{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13160406",
   "metadata": {},
   "source": [
    "# 01 – Storage and Organization (Filesystem and Optional Database)\n",
    "\n",
    "This notebook comes **after** `00_data_acquisition.ipynb`.\n",
    "\n",
    "It documents the storage and organization strategy used in the project and\n",
    "(optionally) demonstrates how the processed data can be loaded into a\n",
    "relational database.\n",
    "\n",
    "Focus:\n",
    "\n",
    "1. **Filesystem-based storage model**\n",
    "   - How data are organized under `data/` (raw vs processed).\n",
    "   - Naming conventions for CSV files and other artifacts.\n",
    "\n",
    "2. **Optional RDBMS loading example (SQLite)**\n",
    "   - How to load the integrated CSV into a SQLite database.\n",
    "   - A simple SQL-style query to illustrate usage.\n",
    "\n",
    "3. **Connection to extraction & enrichment**\n",
    "   - How the integrated dataset produced by `scripts/integrate_data.py`\n",
    "     / `02_integration.ipynb` fits into this storage layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c7b5ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/Users/ujjwal/Downloads/IS-477-Project-Ujjwal'),\n",
       " PosixPath('/Users/ujjwal/Downloads/IS-477-Project-Ujjwal/data'),\n",
       " PosixPath('/Users/ujjwal/Downloads/IS-477-Project-Ujjwal/data/raw'),\n",
       " PosixPath('/Users/ujjwal/Downloads/IS-477-Project-Ujjwal/data/processed'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "PROJECT_ROOT, DATA_DIR, RAW_DIR, PROCESSED_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e617f",
   "metadata": {},
   "source": [
    "## 1. Filesystem structure\n",
    "\n",
    "The project follows a **filesystem-based, tabular storage strategy** instead\n",
    "of a full relational database. All data are stored as CSV files in a\n",
    "structured folder layout inside the Git repository.\n",
    "\n",
    "High-level layout (relative to the project root):\n",
    "\n",
    "```text\n",
    ".\n",
    "├── data/\n",
    "│   ├── raw/          # Raw input data downloaded from Kaggle\n",
    "│   ├── processed/    # Cleaned, integrated, and derived CSV files\n",
    "│   └── README.md     # Instructions for data acquisition & integrity checks\n",
    "├── figures/          # Output plots and visualizations\n",
    "├── notebooks/        # Jupyter notebooks (00_data_acquisition, 01_integration, etc.)\n",
    "├── scripts/          # Python scripts (get_data.py, integrate_data.py, ...)\n",
    "├── ProjectPlan.md\n",
    "├── StatusReport.md\n",
    "└── README.md         # Final project report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31b10e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/ujjwal/Downloads/IS-477-Project-Ujjwal\n",
      "Directory tree under data/:\n",
      "├── processed\n",
      "│   ├── coffee_integrated.csv\n",
      "│   ├── coffee_sales_clean.csv\n",
      "│   └── coffee_shop_clean.csv\n",
      "├── raw\n",
      "│   ├── .DS_Store\n",
      "│   ├── coffee-sales.zip\n",
      "│   ├── coffee-shop.zip\n",
      "│   ├── coffee_sales.csv\n",
      "│   └── coffee_shop.csv\n",
      "├── .DS_Store\n",
      "├── README.md\n",
      "├── checksums.sha256\n",
      "└── coffee_project.sqlite\n"
     ]
    }
   ],
   "source": [
    "def print_tree(start_path: Path, max_depth: int = 3, prefix: str = \"\"):\n",
    "    \"\"\"\n",
    "    Simple recursive directory tree printer (limited depth for readability).\n",
    "    \"\"\"\n",
    "    if max_depth < 0:\n",
    "        return\n",
    "    \n",
    "    items = sorted(start_path.iterdir(), key=lambda p: (p.is_file(), p.name))\n",
    "    for i, item in enumerate(items):\n",
    "        connector = \"└── \" if i == len(items) - 1 else \"├── \"\n",
    "        print(prefix + connector + item.name)\n",
    "        if item.is_dir():\n",
    "            extension = \"    \" if i == len(items) - 1 else \"│   \"\n",
    "            print_tree(item, max_depth=max_depth - 1, prefix=prefix + extension)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(\"Directory tree under data/:\")\n",
    "print_tree(DATA_DIR, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f6027",
   "metadata": {},
   "source": [
    "### 1.1 Naming conventions\n",
    "\n",
    "Within `data/`, the project uses consistent naming conventions:\n",
    "\n",
    "- **Raw data** (downloaded from Kaggle via `scripts/get_data.py`):\n",
    "  - `data/raw/coffee_sales.csv`\n",
    "  - `data/raw/coffee_shop.csv`\n",
    "\n",
    "- **Cleaned data** (produced by profiling/cleaning notebooks/scripts):\n",
    "  - `data/processed/coffee_sales_clean.csv`\n",
    "  - `data/processed/coffee_shop_clean.csv`\n",
    "\n",
    "- **Integrated data** (produced by integration/enrichment):\n",
    "  - `data/processed/coffee_integrated.csv`\n",
    "\n",
    "This follows a `<source>_<stage>.csv` pattern, where:\n",
    "- `<source>` is the dataset name (`coffee_sales`, `coffee_shop`),\n",
    "- `<stage>` is the processing stage (`clean`, `integrated`, etc.).\n",
    "\n",
    "This layout keeps the data lifecycle visible as **raw → processed → integrated**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3eba940c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/ujjwal/Downloads/IS-477-Project-Ujjwal/data/processed/coffee_integrated.csv'),\n",
       " PosixPath('/Users/ujjwal/Downloads/IS-477-Project-Ujjwal/data/processed/coffee_sales_clean.csv'),\n",
       " PosixPath('/Users/ujjwal/Downloads/IS-477-Project-Ujjwal/data/processed/coffee_shop_clean.csv')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_files = list(PROCESSED_DIR.glob(\"*.csv\"))\n",
    "processed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1f44e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee_sales_clean.csv shape: (149116, 11)\n",
      "coffee_shop_clean.csv shape:  (3547, 11)\n",
      "coffee_integrated.csv shape:  (149116, 20)\n"
     ]
    }
   ],
   "source": [
    "sales_clean_path = PROCESSED_DIR / \"coffee_sales_clean.csv\"\n",
    "shop_clean_path = PROCESSED_DIR / \"coffee_shop_clean.csv\"\n",
    "integrated_path = PROCESSED_DIR / \"coffee_integrated.csv\"\n",
    "\n",
    "sales_clean = pd.read_csv(sales_clean_path)\n",
    "shop_clean = pd.read_csv(shop_clean_path)\n",
    "integrated = pd.read_csv(integrated_path)\n",
    "\n",
    "print(\"coffee_sales_clean.csv shape:\", sales_clean.shape)\n",
    "print(\"coffee_shop_clean.csv shape: \", shop_clean.shape)\n",
    "print(\"coffee_integrated.csv shape: \", integrated.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175bcfe",
   "metadata": {},
   "source": [
    "## 2. Storage strategy recap\n",
    "\n",
    "In summary:\n",
    "\n",
    "- The project **does not rely on an external database** for its core workflow.\n",
    "- Instead, it uses:\n",
    "  - Raw CSVs in `data/raw/` (recreated by `scripts/get_data.py` and\n",
    "    documented in `00_data_acquisition.ipynb` and `data/README.md`).\n",
    "  - Cleaned & integrated CSVs in `data/processed/` (recreated by\n",
    "    profiling and integration steps).\n",
    "\n",
    "This approach:\n",
    "\n",
    "- Keeps the project lightweight and easy to run on any machine.\n",
    "- Aligns with the size/complexity of the datasets.\n",
    "- Enhances reproducibility by making each stage visible as files\n",
    "  (`raw` → `clean` → `integrated`).\n",
    "\n",
    "The next section shows an **optional** example of loading the integrated\n",
    "data into a relational database (SQLite), to satisfy the \"RDBMS\" part of\n",
    "the assignment rubric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e21f3",
   "metadata": {},
   "source": [
    "## 3. Optional: loading the integrated data into a relational database (SQLite)\n",
    "\n",
    "The assignment mentions:\n",
    "\n",
    "> Script(s) used to load data into a relational database (if you use a RDBMS).\n",
    "\n",
    "The main pipeline uses a filesystem-based model, but to demonstrate how the\n",
    "data could be used with a relational database, we will:\n",
    "\n",
    "1. Create a SQLite database file under `data/`.\n",
    "2. Load `coffee_integrated.csv` into a SQL table using `pandas.to_sql`.\n",
    "3. Run a simple SQL query to summarize the data.\n",
    "\n",
    "This section is **illustrative** and not required for the core\n",
    "filesystem-based analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a869b5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/ujjwal/Downloads/IS-477-Project-Ujjwal/data/coffee_project.sqlite')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to a SQLite database file (will be created if not present)\n",
    "sqlite_path = DATA_DIR / \"coffee_project.sqlite\"\n",
    "\n",
    "# Connect to SQLite\n",
    "conn = sqlite3.connect(sqlite_path)\n",
    "\n",
    "# Make a copy of the integrated DataFrame and rename potentially conflicting columns\n",
    "integrated_db = integrated.copy()\n",
    "\n",
    "# Rename weekday-related columns to avoid conflicts in SQLite\n",
    "rename_map = {}\n",
    "if \"weekday\" in integrated_db.columns:\n",
    "    rename_map[\"weekday\"] = \"weekday_sales\"\n",
    "if \"Weekday\" in integrated_db.columns:\n",
    "    rename_map[\"Weekday\"] = \"weekday_shop\"\n",
    "\n",
    "if rename_map:\n",
    "    integrated_db = integrated_db.rename(columns=rename_map)\n",
    "\n",
    "# Ensure a revenue column exists for the later SQL example\n",
    "if \"revenue\" not in integrated_db.columns and {\"transaction_qty\", \"unit_price\"}.issubset(integrated_db.columns):\n",
    "    integrated_db[\"revenue\"] = integrated_db[\"transaction_qty\"] * integrated_db[\"unit_price\"]\n",
    "\n",
    "# Write the DataFrame to a SQL table named 'coffee_integrated'\n",
    "integrated_db.to_sql(\"coffee_integrated\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "sqlite_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "095b1d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_location</th>\n",
       "      <th>avg_revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lower Manhattan</td>\n",
       "      <td>4.814726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hell's Kitchen</td>\n",
       "      <td>4.661696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astoria</td>\n",
       "      <td>4.589891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    store_location  avg_revenue\n",
       "0  Lower Manhattan     4.814726\n",
       "1   Hell's Kitchen     4.661696\n",
       "2          Astoria     4.589891"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example query: average revenue per store_location, top 10\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT store_location,\n",
    "       AVG(revenue) AS avg_revenue\n",
    "FROM coffee_integrated\n",
    "GROUP BY store_location\n",
    "ORDER BY avg_revenue DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "avg_rev_by_store = pd.read_sql(query, conn)\n",
    "avg_rev_by_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6d41c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8dadaa",
   "metadata": {},
   "source": [
    "## 4. Connection to extraction and enrichment\n",
    "\n",
    "The integrated dataset used above (`coffee_integrated.csv`) is produced by\n",
    "the extraction/enrichment steps in the project:\n",
    "\n",
    "- **Script**: `scripts/integrate_data.py`\n",
    "  - Derives `hour_of_day` from `transaction_time` in the sales data.\n",
    "  - Aggregates the coffee shop data by `hour_of_day` to create an hourly profile.\n",
    "  - Merges the two datasets on `hour_of_day` to create `coffee_integrated.csv`.\n",
    "\n",
    "- **Notebook**: `02_integration.ipynb`\n",
    "  - Documents and visualizes the same integration logic with sanity checks\n",
    "    and basic summaries.\n",
    "\n",
    "From a storage perspective:\n",
    "\n",
    "- The integrated CSV is stored under `data/processed/` following the\n",
    "  project’s naming conventions.\n",
    "- It can be:\n",
    "  - Used directly as a CSV for analysis (e.g., in a profiling/EDA notebook),\n",
    "  - Or loaded into a relational database (as shown above) for SQL-style\n",
    "    querying or integration with other systems.\n",
    "\n",
    "This notebook therefore completes the documentation for:\n",
    "\n",
    "- **Storage and organization** – filesystem layout and naming conventions.\n",
    "- **RDBMS loading (optional)** – how the data can be represented in a\n",
    "  relational database.\n",
    "- The link between storage and **extraction & enrichment**, by showing\n",
    "  where the integrated dataset lives and how it can be consumed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
