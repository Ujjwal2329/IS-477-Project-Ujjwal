# Data directory

This folder contains all data used in the project.
This folder contains all data used in the project. 
The project treats `data/` as the main storage location for all tabular data.
Raw Kaggle downloads live in `data/raw/`, while cleaned and integrated CSVs
produced by the pipeline live in `data/processed/`. All files use a consistent
`<source>_<stage>.csv` naming convention.

- `raw/` – **raw input data** downloaded from Kaggle using `scripts/get_data.py`
- `processed/` – **cleaned / integrated data** produced by notebooks or scripts
- `checksums.sha256` – SHA-256 hashes for the raw input files

Raw data files are **not** stored in the Git repo; they are recreated
programmatically.

---

## Datasets

This project uses two public Kaggle datasets:

1. **Coffee Sales dataset**  
   Kaggle ID: `ahmedabbas757/coffee-sales`  
   Saved locally as: `data/raw/coffee_sales.csv`

2. **Coffee Shop dataset**  
   Kaggle ID: `jawad3664/coffee-shop`  
   Saved locally as: `data/raw/coffee_shop.csv`

The script `scripts/get_data.py` downloads both and unzips them into
`data/raw/`.

---

## Prerequisites: Kaggle API setup

1. Create a Kaggle account and go to **Account → Settings → API**.
2. Either:

   - **Recommended (API v1.8+)**: create a new API token and set it as
     an environment variable in your shell:

     ```bash
     export KAGGLE_API_TOKEN='KGAT_...your_token_here...'
     ```

     (You can add that line to `~/.zshrc` or `~/.bashrc` to make it
     permanent.)

   - **Legacy method**: click **“Create New Token”** under *Legacy API
     Credentials*. This downloads `kaggle.json`. Move it to:

     - macOS/Linux: `~/.kaggle/kaggle.json`
     - Windows: `C:\Users\<USER>\.kaggle\kaggle.json`

     On macOS/Linux you may need:

     ```bash
     chmod 600 ~/.kaggle/kaggle.json
     ```

3. From the **repository root**, create and activate a virtual
   environment (optional but recommended) and install the Kaggle client:

   ```bash
   python -m venv .venv
   source .venv/bin/activate  # or .venv\Scripts\Activate.ps1 on Windows
   pip install kaggle

4. Test that the Kaggle CLI is working by listing some datasets:

   kaggle datasets list -s "coffee"

5. Download the raw data used in this project. From the **root of this repository**, run:

   python scripts/get_data.py download

   This will:
   - Download the two Kaggle datasets into `data/raw/`
   - Unzip them, producing:
     - `data/raw/coffee_sales.csv`
     - `data/raw/coffee_shop.csv`

   These files are the inputs used by the notebooks (e.g. `notebooks/01_profiling.ipynb`).

6. (For the main project copy) compute and save SHA-256 checksums for the raw files:

   python scripts/get_data.py write-checks

   This creates `data/checksums.sha256` containing lines of the form:

   <sha256-hash>  data/raw/coffee_sales.csv
   <sha256-hash>  data/raw/coffee_shop.csv

   This file is committed to the repository so others can verify that they are using
   the exact same input data.

7. On any machine that wants to reproduce the project, verify data integrity by running:

   python scripts/get_data.py verify

   If all hashes match the entries in `data/checksums.sha256`, you will see OK messages
   for each file. Any missing files or mismatches will be reported as failures.

Processed data:
- Scripts and notebooks in this project write cleaned and integrated data to `data/processed/`, for example:
  - `data/processed/coffee_sales_clean.csv`
  - `data/processed/coffee_shop_clean.csv`
  - (later) `data/processed/coffee_integrated.csv`
- These files are derived from the raw Kaggle data and can be regenerated by re-running
  the workflow described in the main project README.